{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import spacy\n",
    "import string\n",
    "import re\n",
    "import wordcloud\n",
    "import matplotlib.pyplot as plt\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:powderblue;\">General helper Classes</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleaningHelper():\n",
    "    \"\"\"Cleaning Helper\"\"\"\n",
    "    def __init__(self, version):\n",
    "        self.version = version\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"Cleaning helper version {self.version}\"\n",
    "    \n",
    "    def get_nulls_data(self, df):\n",
    "        #We want to know the quality of data. So, let's start by detecting not null percentage related to every column. \n",
    "\n",
    "        df_tot_nulls = df.isnull().sum().sort_values(ascending=False)\n",
    "        df_tot_nulls_perc = 100 - round(df_tot_nulls/len(df)*100,2)\n",
    "        df_tot_perc_nulls = pd.concat([df_tot_nulls,df_tot_nulls_perc],axis=1)\n",
    "        df_tot_perc_nulls = df_tot_perc_nulls.rename(columns={0: \"Total\", 1: \"PercNotNull\"})\n",
    "        return df_tot_perc_nulls\n",
    "    \n",
    "    def get_url_str(self, token_):\n",
    "        regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
    "        url = re.findall(regex,token_)\n",
    "        return [x[0] for x in url]\n",
    "    \n",
    "    def get_twitter_username_str(self, token_):\n",
    "        regex = r\"(?<=^|(?<=[^a-zA-Z0-9-_\\.]))@([A-Za-z]+[A-Za-z0-9-_]+)\"\n",
    "        username = re.findall(regex,token_)\n",
    "        return [x[0] for x in username]\n",
    "\n",
    "    def get_custom_stop_words(self, spacy_):\n",
    "        spacy_stopwords = spacy_.Defaults.stop_words\n",
    "        \n",
    "        with open(\"stopwords.txt\") as file:\n",
    "            more_stopwords = {line.rstrip() for line in file}\n",
    "        \n",
    "        final_stopwords = spacy_stopwords | more_stopwords\n",
    "        \n",
    "        return final_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleaningTweets():\n",
    "    \"\"\"Cleaning Tweets\"\"\"\n",
    "    def __init__(self, version, spacy_, parser_, punctuation_str_, stop_words_):\n",
    "        self.version          = version\n",
    "        self.spacy_           = spacy_\n",
    "        self.parser_          = parser_\n",
    "        self.punctuation_str_ = punctuation_str_\n",
    "        self.stop_words_      = stop_words_\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"Cleaning Tweets version {self.version}\"\n",
    "    \n",
    "    def do_spacy_tokenizer(self, token_):\n",
    "        #https://spacy.io/api/annotation\n",
    "        \n",
    "        # initializing CleaningHelper class\n",
    "        cleaningHelper = CleaningHelper(version = \"1.0\")\n",
    "        \n",
    "        # initializing Token's doc\n",
    "        doc_tokens = self.parser_(token_)\n",
    "        \n",
    "        # Removing twitter - username \n",
    "        doc_tokens = [ token_ for token_ in doc_tokens if len(cleaningHelper.get_twitter_username_str(token_ = token_.lemma_.lower().strip())) == 0 ]\n",
    "\n",
    "        # Removing token - URL \n",
    "        doc_tokens = [ token_ for token_ in doc_tokens if len(cleaningHelper.get_url_str(token_ = token_.lemma_.lower().strip())) == 0 ]\n",
    "\n",
    "        # Lemmatizing each token and converting each token into lowercase\n",
    "        doc_tokens = [ token_.lemma_.lower().strip() if token_.lemma_ != \"-PRON-\" else token_.lower_ for token_ in doc_tokens ]\n",
    "\n",
    "        # Removing stop words\n",
    "        doc_tokens = [ token_ for token_ in doc_tokens if token_ not in stop_words and token_ not in punctuation_str ]\n",
    "        \n",
    "        return doc_tokens\n",
    "    \n",
    "    def get_words_df(self, df_Tweets_):\n",
    "        #df_doc_tokens = pd.DataFrame(columns = ['token_'])\n",
    "        df_doc_tokens = pd.DataFrame({'token_': pd.Series([], dtype='str')})\n",
    "        for index, row in df_Tweets_.iterrows():\n",
    "            token_ = str(row[\"tweet\"], encoding='UTF-8')\n",
    "            doc_tokens = self.do_spacy_tokenizer(token_ = token_)\n",
    "            #print(doc_tokens)\n",
    "            for doc_token in doc_tokens:\n",
    "                df_doc_tokens = df_doc_tokens.append({'token_' : doc_token}, ignore_index = True)\n",
    "        return df_doc_tokens\n",
    "    \n",
    "    def get_words_list(self, df_Tweets_):\n",
    "        words_list =[]\n",
    "        for index, row in df_Tweets_.iterrows():\n",
    "            token_ = str(row[\"token_\"])\n",
    "            words_list.append(token_)\n",
    "        return words_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsfinalproject]",
   "language": "python",
   "name": "conda-env-dsfinalproject-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
