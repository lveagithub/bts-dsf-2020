{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import spacy\n",
    "import string\n",
    "import re\n",
    "import wordcloud\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import multiprocessing\n",
    "from unidecode import unidecode\n",
    "import math\n",
    "\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from IPython.display import display\n",
    "\n",
    "import logging  # Setting up the loggings to monitor processes\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:powderblue;\">General helper Classes</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleaningHelper():\n",
    "    \"\"\"Cleaning Helper\"\"\"\n",
    "    def __init__(self, version):\n",
    "        self.version = version\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"Cleaning helper version {self.version}\"\n",
    "    \n",
    "    def get_nulls_data(self, df):\n",
    "        #We want to know the quality of data. So, let's start by detecting not null percentage related to every column. \n",
    "\n",
    "        df_tot_nulls = df.isnull().sum().sort_values(ascending=False)\n",
    "        df_tot_nulls_perc = 100 - round(df_tot_nulls/len(df)*100,2)\n",
    "        df_tot_perc_nulls = pd.concat([df_tot_nulls,df_tot_nulls_perc],axis=1)\n",
    "        df_tot_perc_nulls = df_tot_perc_nulls.rename(columns={0: \"Total\", 1: \"PercNotNull\"})\n",
    "        return df_tot_perc_nulls\n",
    "    \n",
    "    def get_url_str(self, token_):\n",
    "        regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
    "        url = re.findall(regex,token_)\n",
    "        return [x[0] for x in url]\n",
    "    \n",
    "    def get_twitter_username_str(self, token_):\n",
    "        regex = r\"(?<=^|(?<=[^a-zA-Z0-9-_\\.]))@([A-Za-z]+[A-Za-z0-9-_]+)\"\n",
    "        username = re.findall(regex,token_)\n",
    "        return [x[0] for x in username]\n",
    "\n",
    "    def get_custom_stop_words(self, spacy_):\n",
    "        spacy_stopwords = spacy_.Defaults.stop_words\n",
    "        \n",
    "        with open(\"stopwords.txt\") as file:\n",
    "            more_stopwords = {line.rstrip() for line in file}\n",
    "        \n",
    "        final_stopwords = spacy_stopwords | more_stopwords\n",
    "        \n",
    "        return final_stopwords\n",
    "    \n",
    "    def get_custom_rate(self, rate_):\n",
    "        custom_rate = 0\n",
    "        if rate_ > 0:\n",
    "            custom_rate = 1\n",
    "        return custom_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleaningTweets():\n",
    "    \"\"\"Cleaning Tweets\"\"\"\n",
    "    def __init__(self, version, spacy_, parser_, punctuation_str_, stop_words_):\n",
    "        self.version          = version\n",
    "        self.spacy_           = spacy_\n",
    "        self.parser_          = parser_\n",
    "        self.punctuation_str_ = punctuation_str_\n",
    "        self.stop_words_      = stop_words_\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"Cleaning Tweets version {self.version}\"\n",
    "    \n",
    "    def do_spacy_tokenizer(self, token_):\n",
    "        #https://spacy.io/api/annotation\n",
    "        \n",
    "        # initializing CleaningHelper class\n",
    "        cleaningHelper = CleaningHelper(version = \"1.0\")\n",
    "        \n",
    "        # initializing Token's doc\n",
    "        doc_tokens = self.parser_(token_)\n",
    "        \n",
    "        # Removing twitter - username \n",
    "        doc_tokens = [ token_ for token_ in doc_tokens if len(cleaningHelper.get_twitter_username_str(token_ = token_.lemma_.lower().strip())) == 0 ]\n",
    "\n",
    "        # Removing token - URL \n",
    "        doc_tokens = [ token_ for token_ in doc_tokens if len(cleaningHelper.get_url_str(token_ = token_.lemma_.lower().strip())) == 0 ]\n",
    "\n",
    "        # Lemmatizing each token and converting each token into lowercase\n",
    "        doc_tokens = [ token_.lemma_.lower().strip() if token_.lemma_ != \"-PRON-\" else token_.lower_ for token_ in doc_tokens ]\n",
    "\n",
    "        # Removing stop words\n",
    "        doc_tokens = [ token_ for token_ in doc_tokens if token_ not in stop_words and token_ not in punctuation_str ]\n",
    "        \n",
    "        return doc_tokens\n",
    "    \n",
    "    def get_words_df(self, df_Tweets_):\n",
    "        #df_doc_tokens = pd.DataFrame(columns = ['token_'])\n",
    "        df_doc_tokens = pd.DataFrame({'token_': pd.Series([], dtype='str')})\n",
    "        for index, row in df_Tweets_.iterrows():\n",
    "            token_ = str(row[\"tweet\"], encoding='UTF-8')\n",
    "            doc_tokens = self.do_spacy_tokenizer(token_ = token_)\n",
    "            #print(doc_tokens)\n",
    "            for doc_token in doc_tokens:\n",
    "                df_doc_tokens = df_doc_tokens.append({'token_' : doc_token}, ignore_index = True)\n",
    "        return df_doc_tokens\n",
    "    \n",
    "    def get_words_list(self, df_Tweets_):\n",
    "        words_list =[]\n",
    "        for index, row in df_Tweets_.iterrows():\n",
    "            token_ = str(row[\"token_\"])\n",
    "            words_list.append(token_)\n",
    "        return words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalysis():\n",
    "    \"\"\"Sentiment Analysis\"\"\"\n",
    "    def __init__(self, version):\n",
    "        self.version = version\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"Sentiment Analysis version {self.version}\"\n",
    "    \n",
    "    def create_tfidf_dictionary(self, x, transformed_sentences, features):\n",
    "        '''\n",
    "        create dictionary for each input sentence x, where each word has assigned its tfidf score\n",
    "\n",
    "        inspired  by function from this wonderful article: \n",
    "        https://medium.com/analytics-vidhya/automated-keyword-extraction-from-articles-using-nlp-bfd864f41b34\n",
    "\n",
    "        x - row of dataframe, containing sentences, and their indexes,\n",
    "        transformed_sentences - all sentences transformed with TfidfVectorizer\n",
    "        features - names of all words in corpus used in TfidfVectorizer\n",
    "\n",
    "        '''\n",
    "        vector_coo = transformed_sentences[x.name].tocoo()\n",
    "        vector_coo.col = features.iloc[vector_coo.col].values\n",
    "        dict_from_coo = dict(zip(vector_coo.col, vector_coo.data))\n",
    "        return dict_from_coo\n",
    "\n",
    "    def replace_tfidf_words(self, x, transformed_sentences, features):\n",
    "        '''\n",
    "        replacing each word with it's calculated tfidf dictionary with scores of each word\n",
    "        x - row of dataframe, containing sentences, and their indexes,\n",
    "        transformed_sentences - all sentences transformed with TfidfVectorizer\n",
    "        features - names of all words in corpus used in TfidfVectorizer\n",
    "        '''\n",
    "        dictionary = self.create_tfidf_dictionary(x, transformed_sentences, features)   \n",
    "        return list(map(lambda y:dictionary[f'{y}'], x.tweet_vector.split()))\n",
    "\n",
    "    def replace_sentiment_words(self, word, sentiment_dict):\n",
    "        '''\n",
    "        replacing each word with its associated sentiment score from sentiment dict\n",
    "        '''\n",
    "        try:\n",
    "            out = sentiment_dict[word]\n",
    "        except KeyError:\n",
    "            out = 0\n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsfinalproject]",
   "language": "python",
   "name": "conda-env-dsfinalproject-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
